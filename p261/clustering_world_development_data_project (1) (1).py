# -*- coding: utf-8 -*-
"""Clustering_World_Development_Data_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-Ptm7j6gYAVSGgPx8p2lt3ZMXpuDA_f6
"""

import pandas as pd
import numpy as np 
import seaborn as sns 
import matplotlib.pyplot as plt
import sklearn
import scipy as sci
import re
import warnings
warnings.filterwarnings("ignore")
from sklearn.preprocessing import MinMaxScaler
from sklearn.impute import KNNImputer
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.metrics import mean_squared_error
from sklearn.cluster import DBSCAN
from scipy.cluster.hierarchy import linkage, dendrogram
from sklearn.metrics import calinski_harabasz_score
from sklearn.cluster import AgglomerativeClustering
from sklearn.decomposition import PCA
from sklearn.model_selection import GridSearchCV

df=pd.read_excel('World_development_mesurement.xlsx')
df

df.shape

df.info()

df.describe()

df.isnull().sum()

df['GDP']=df['GDP'].str.replace(',','')
df['GDP']=df['GDP'].str.replace('$','')
df['GDP']=df['GDP'].astype(float)

df['Business Tax Rate']=df['Business Tax Rate'].str.replace(',','')
df['Business Tax Rate']=df['Business Tax Rate'].str.replace('%','')
df['Business Tax Rate']=df['Business Tax Rate'].astype(float)

df['Health Exp/Capita']=df['Health Exp/Capita'].str.replace(',','')
df['Health Exp/Capita']=df['Health Exp/Capita'].str.replace('$','')
df['Health Exp/Capita']=df['Health Exp/Capita'].astype(float)

df['Tourism Inbound']=df['Tourism Inbound'].str.replace(',','')
df['Tourism Inbound']=df['Tourism Inbound'].str.replace('$','')
df['Tourism Inbound']=df['Tourism Inbound'].astype(float)

df['Tourism Outbound']=df['Tourism Outbound'].str.replace(',','')
df['Tourism Outbound']=df['Tourism Outbound'].str.replace('$','')
df['Tourism Outbound']=df['Tourism Outbound'].astype(float)

df

for i in df.columns :
  if i!="Country":
    df[i] = df[i].apply(lambda x: re.sub("[$,%]","",x) if isinstance(x, str) and x is not None else x)
    df[i] = pd.to_numeric(df[i],errors='coerce')
    df[i]

# dropping number of records column why because all are '1' values
df1=df.drop("Number of Records",axis=1)
df1

df2=df1.drop("Country",axis=1)
df2

# copying the dataframe
df3=df2

df1[df1.duplicated()].shape

df1[df1.duplicated()]

df3['Birth Rate']=df3['Birth Rate'].fillna(df3['Birth Rate'].mean())
df3['Business Tax Rate']=df3['Business Tax Rate'].fillna(df3['Business Tax Rate'].mean())
df3['CO2 Emissions']=df3['CO2 Emissions'].fillna(df3['CO2 Emissions'].mean())
df3['Days to Start Business']=df3['Days to Start Business'].fillna(df3['Days to Start Business'].mean())
df3['Ease of Business']=df3['Ease of Business'].fillna(df3['Ease of Business'].mean())
df3['Energy Usage']=df3['Energy Usage'].fillna(df3['Energy Usage'].mean())
df3['GDP']=df3['GDP'].fillna(df3['GDP'].mean())
df3['Health Exp % GDP']=df3['Health Exp % GDP'].fillna(df3['Health Exp % GDP'].mean())
df3['Health Exp/Capita']=df3['Health Exp/Capita'].fillna(df3['Health Exp/Capita'].mean())
df3['Hours to do Tax']=df3['Hours to do Tax'].fillna(df3['Hours to do Tax'].mean())
df3['Infant Mortality Rate']=df3['Infant Mortality Rate'].fillna(df3['Infant Mortality Rate'].mean())
df3['Internet Usage']=df3['Internet Usage'].fillna(df3['Internet Usage'].mean())
df3['Lending Interest']=df3['Lending Interest'].fillna(df3['Lending Interest'].mean())
df3['Life Expectancy Female']=df3['Life Expectancy Female'].fillna(df3['Life Expectancy Female'].mean())
df3['Life Expectancy Male']=df3['Life Expectancy Male'].fillna(df3['Life Expectancy Male'].mean())
df3['Mobile Phone Usage']=df3['Mobile Phone Usage'].fillna(df3['Mobile Phone Usage'].mean())
df3['Population 0-14']=df3['Population 0-14'].fillna(df3['Population 0-14'].mean())
df3['Population 15-64']=df3['Population 15-64'].fillna(df3['Population 15-64'].mean())
df3['Population 65+']=df3['Population 65+'].fillna(df3['Population 65+'].mean())
df3['Population Total']=df3['Population Total'].fillna(df3['Population Total'].mean())
df3['Population Urban']=df3['Population Urban'].fillna(df3['Population Urban'].mean())
df3['Tourism Inbound']=df3['Tourism Inbound'].fillna(df3['Tourism Inbound'].mean())
df3['Tourism Outbound']=df3['Tourism Outbound'].fillna(df3['Tourism Outbound'].mean())
df3

df3.isnull().sum()

# imputation of null values by KNN imputer
knn_imputer = KNNImputer(n_neighbors=5)
imputed_df = pd.DataFrame(knn_imputer.fit_transform(df2),columns=df2.columns)
imputed_df.head()

imputed_df.isnull().sum()

# To check unique countries
len(df["Country"].unique())

df4=pd.concat([df["Country"], imputed_df], axis=1)
df4

# here we group the country by mean giving more weight to recent value or recent year
weight = [0.1,0.2,0.3,0.4,0.5,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.92]
weighted_mean = lambda x : np.average(x, weights=weight)
df5=df4.groupby('Country')[[i for i in df4.columns if i!='Country']].agg(weighted_mean).reset_index()
df5

df5.shape

def plot(x,**kwarge):
  fig, ax = plt.subplots(1, 2, figsize=(20,4))
  sns.distplot(x, ax=ax[0],kde=True)
  sns.boxplot(x, ax=ax[1])
  plt.show()
for i in df4.columns:
  if i!="Country":
    plot(df4[i])
  else:
    continue

df6=pd.concat([df["Country"], df3], axis=1)
df6

df.corr()

corr_matrix=df6.corr()

#Format the plot background and scatter plots for all the variables
sns.set_style(style='darkgrid')
sns.pairplot(df)

plt.figure(figsize=(12,8))
sns.heatmap(corr_matrix,annot=True,cmap='coolwarm')
plt.show()

pca = PCA(n_components = 25)
pca_values = pca.fit_transform(df_norm1)
pca_values

#loadings or weights
pca.components_

# The amount of variance that each PCA explains is 
var = pca.explained_variance_ratio_
var

# Cumulative variance 
var1 = np.cumsum(np.round(var,decimals = 4)*100)
var1

# Variance plot for PCA components obtained 
plt.plot(var1,color="red")

finalDf = pd.concat([pd.DataFrame(pca_values[:,0:2],columns=['pc1','pc2']), df6['Country']], axis = 1)
finalDf

import matplotlib.pyplot as plt
plt.style.use('classic')

import seaborn as sns
sns.scatterplot(data=finalDf,x='pc1',y='pc2',hue='Country',s = 100)

array=df2.values
array

def norm_func(i):
    x = (i-i.min()) / (i.max() - i.min())
    return (x)

df_norm=norm_func(df6.iloc[:,1:])
df_norm

from sklearn.preprocessing import MinMaxScaler
trans=MinMaxScaler()
df_norm1=pd.DataFrame(trans.fit_transform(df6.iloc[:,1:]))
df_norm1

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
df_stand = pd.DataFrame(scaler.fit_transform(df6.iloc[:,1:]))
df_stand

from scipy.cluster.hierarchy import linkage 
import scipy.cluster.hierarchy as sch # for creating dendrogram 
#p = np.array(df_norm) # converting into numpy array format 
z = linkage(df_norm1, method="ward",metric="euclidean")
plt.figure(figsize=(15, 5))
plt.title('Hierarchical Clustering Dendrogram')
plt.xlabel('Index')
plt.ylabel('Distance')
sch.dendrogram(
    z,
    #leaf_rotation=6.,  # rotates the x axis labels
    #leaf_font_size=15.,  # font size for the x axis labels
)
plt.show()

from sklearn.cluster import AgglomerativeClustering 
import warnings 
warnings.filterwarnings('ignore')
h_complete = AgglomerativeClustering(n_clusters=6, linkage='ward',affinity = "euclidean").fit(df_norm1) 

cluster_labels=pd.Series(h_complete.labels_)
cluster_labels
df6['clust']=cluster_labels # creating a  new column and assigning it to new column 
df6

df6.iloc[:,1:].groupby(df6.clust).mean()

from scipy.cluster.hierarchy import linkage 
import scipy.cluster.hierarchy as sch # for creating dendrogram 
#p = np.array(df_norm) # converting into numpy array format 
z = linkage(df_stand, method="ward",metric="euclidean")
plt.figure(figsize=(15, 5))
plt.title('Hierarchical Clustering Dendrogram')
plt.xlabel('Index')
plt.ylabel('Distance')
sch.dendrogram(
    z,
    #leaf_rotation=6.,  # rotates the x axis labels
    #leaf_font_size=15.,  # font size for the x axis labels
)
plt.show()

from sklearn.cluster import AgglomerativeClustering 
import warnings 
warnings.filterwarnings('ignore')
h_complete = AgglomerativeClustering(n_clusters=6, linkage='ward',affinity = "euclidean").fit(df_stand) 

cluster_labels=pd.Series(h_complete.labels_)
cluster_labels
df6['clust']=cluster_labels # creating a  new column and assigning it to new column 
df6

df6.iloc[:,1:].groupby(df6.clust).mean()

from sklearn.cluster import KMeans
fig = plt.figure(figsize=(10, 8))
WCSS = []
for i in range(1, 11):
    clf = KMeans(n_clusters=i)
    clf.fit(df_norm)
    WCSS.append(clf.inertia_) # inertia is another name for WCSS
plt.plot(range(1, 11), WCSS)
plt.title('The Elbow Method')
plt.ylabel('WCSS')
plt.xlabel('Number of Clusters')
plt.show()

clf = KMeans(n_clusters=6)
y_kmeans = clf.fit_predict(df_norm1)

from sklearn.neighbors import NearestNeighbors

neigh = NearestNeighbors(n_neighbors=48)
nbrs = neigh.fit(df_stand)
distances, indices = nbrs.kneighbors(df_stand)

distances = np.sort(distances, axis=0)
distances = distances[:,1]
plt.plot(distances)

dbscan = DBSCAN(eps=3.0, min_samples=48)
dbscan.fit(df_stand)

#Noisy samples are given the label -1.
dbscan.labels_

cl=pd.DataFrame(dbscan.labels_,columns=['cluster'])

cl
pd.set_option("display.max_rows", None)

cl

data = pd.concat([df6,cl],axis=1)  
data

dl = dbscan.labels_

import sklearn
sklearn.metrics.silhouette_score(df_stand, dl)

dbscan = DBSCAN(eps=3.5, min_samples=50)
dbscan.fit(df_stand)

dbscan.labels_

dl1 = dbscan.labels_

import sklearn
sklearn.metrics.silhouette_score(df_stand, dl1)

dbscan = DBSCAN(eps=3.57, min_samples=55)
dbscan.fit(df_stand)

dbscan.labels_

dl2 = dbscan.labels_

import sklearn
sklearn.metrics.silhouette_score(df_stand, dl2)

dbscan = DBSCAN(eps=4.58, min_samples=60)
dbscan.fit(df_stand)

dbscan.labels_

dl3 = dbscan.labels_

import sklearn
sklearn.metrics.silhouette_score(df_stand, dl3)